## Task 4: Conceptual Questions

### Question 1: What is entropy and information gain?

**Answer:**
- Entropy measures the impurity or disorder in a dataset — higher entropy means more mixed classes.
- Information Gain is the reduction in entropy when a dataset is split — it shows how much a feature helps in making the data more pure.

---

### Question 2: Difference between Gini Index and Entropy

**Answer:**

- Both are used to measure how "pure" a split is.
- Gini Index is faster and simpler, focuses on misclassification.
- Entropy is more math-heavy, based on information theory. Both usually give similar results.

---

### Question 3: How can a decision tree overfit? How to avoid it?

**Answer:**

- A decision tree can overfit by growing too deep and memorizing noise in the training data.
- You can avoid it by limiting depth, setting a     minimum number of samples per leaf, or using pruning techniques.


---

